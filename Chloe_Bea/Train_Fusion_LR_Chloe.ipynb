{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 0\n",
    "%matplotlib notebook\n",
    "import os, sys\n",
    "import logging\n",
    "import random\n",
    "import h5py\n",
    "import shutil\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import sigpy.plot as pl\n",
    "import torch\n",
    "import sigpy as sp\n",
    "import torchvision\n",
    "from torch import optim\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib\n",
    "# import custom libraries\n",
    "from utils import transforms as T\n",
    "from utils import subsample as ss\n",
    "from utils import complex_utils as cplx\n",
    "from utils.resnet2p1d import generate_model\n",
    "from utils.flare_utils import roll\n",
    "# import custom classes\n",
    "from utils.datasets_Chloe import SliceData\n",
    "from subsample_fastmri import MaskFunc\n",
    "from MoDL_single import UnrolledModel\n",
    "import argparse\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from recon_net_wrap import ViTfuser\n",
    "#from UnrolledViT import UnrolledViT\n",
    "from UnrolledViT import UnrolledViT\n",
    "\n",
    "from fastmri.data import transforms, subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Namespace:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransform:\n",
    "    \"\"\"\n",
    "    Data Transformer for training unrolled reconstruction models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mask_func, args, use_seed=False):\n",
    "        self.mask_func = mask_func\n",
    "        self.use_seed = use_seed\n",
    "        self.rng = np.random.RandomState()\n",
    "    def get_mask_func(self, factor):\n",
    "        center_fractions = 0.1 * 4/factor# RandomMaskFuncEquiSpacedMaskFunc\n",
    "        mask_func = subsample.EquiSpacedMaskFunc(\n",
    "        center_fractions=[center_fractions],\n",
    "        accelerations=[factor], \n",
    "        )\n",
    "        return mask_func\n",
    "    \n",
    "    def __call__(self, kspace, target, reference_kspace, reference,slice):\n",
    "        im_lowres = abs(sp.ifft(sp.resize(sp.resize(kspace,(256,24)),(256,160))))\n",
    "        magnitude_vals = im_lowres.reshape(-1)\n",
    "        k = int(round(0.05 * magnitude_vals.shape[0]))\n",
    "        scale = magnitude_vals[magnitude_vals.argsort()[::-1][k]]\n",
    "        kspace = kspace/scale\n",
    "        target = target/scale\n",
    "        # Convert everything from numpy arrays to tensors\n",
    "        kspace_torch = cplx.to_tensor(kspace).float()   \n",
    "        target_torch = cplx.to_tensor(target).float()  \n",
    "        target_torch = T.ifft2(T.fft2(target_torch)) \n",
    "        # Use poisson mask instead\n",
    "        #mask2 = sp.mri.poisson((256,160), 5, calib=(18, 14), dtype=float, crop_corner=False, return_density=True, seed=0, max_attempts=6, tol=0.01)\n",
    "        #mask2[128-10:128+9,80-8:80+7] = 1\n",
    "        #mask_torch = torch.stack([torch.tensor(mask2).float(),torch.tensor(mask2).float()],dim=2)\n",
    "        #mask_torch = T.kspace_crop(mask_torch,0.67)\n",
    "        #kspace_torch = T.kspace_cut(mask_torch,0.5)\n",
    "        kspace_torch = T.awgn_torch(kspace_torch,40,L=1) # 10dB for simulations\n",
    "        ## Masking\n",
    "        mask_func = self.get_mask_func(3)\n",
    "        kspace_torch = transforms.apply_mask(kspace_torch, mask_func)[0]\n",
    "        # kspace_torch = kspace_torch*mask_torch # For poisson\n",
    "        \n",
    "        mask = np.abs(cplx.to_numpy(kspace_torch))!=0\n",
    "        mask_torch = torch.stack([torch.tensor(mask).float(),torch.tensor(mask).float()],dim=2)\n",
    "        \n",
    "        ### Reference addition ###\n",
    "        im_lowres_ref = abs(sp.ifft(sp.resize(sp.resize(reference_kspace,(172,24)),(172,108))))\n",
    "        magnitude_vals_ref = im_lowres_ref.reshape(-1)\n",
    "        k_ref = int(round(0.05 * magnitude_vals_ref.shape[0]))\n",
    "        scale_ref = magnitude_vals_ref[magnitude_vals_ref.argsort()[::-1][k_ref]]\n",
    "        reference = reference / scale_ref\n",
    "        reference_torch = cplx.to_tensor(reference).float()\n",
    "        reference_torch_kspace = T.fft2(reference_torch)\n",
    "        reference_torch_kspace = reference_torch_kspace\n",
    "        reference_torch = T.ifft2(reference_torch_kspace)\n",
    "        \n",
    "\n",
    "        return kspace_torch,target_torch,mask_torch, reference_torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(args):\n",
    "    # Generate k-t undersampling masks\n",
    "    train_mask = MaskFunc([0.08],[4])\n",
    "    train_data = SliceData(\n",
    "        root=str(args.data_path),\n",
    "        transform=DataTransform(train_mask, args),\n",
    "        sample_rate=1\n",
    "    )\n",
    "    return train_data\n",
    "def create_data_loaders(args):\n",
    "    train_data = create_datasets(args)\n",
    "#     print(train_data[0])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_data,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return train_loader\n",
    "def build_optim(args, params):\n",
    "    optimizer = torch.optim.Adam(params, lr=args.lr, weight_decay=args.weight_decay)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper parameters\n",
    "params = Namespace()\n",
    "#params.data_path = \"./registered_data/patient23b/\"\n",
    "params.data_path = \"./RegisteredData/\"\n",
    "params.batch_size = 2 #4\n",
    "params.num_grad_steps = 1 #4\n",
    "params.num_cg_steps = 8 #8\n",
    "params.share_weights = True\n",
    "params.modl_lamda = 0.05\n",
    "params.lr = 0.0001 #0.0005\n",
    "#params.lr = 0.0001\n",
    "params.weight_decay = 0\n",
    "params.lr_step_size = 10\n",
    "params.lr_gamma = 0.3\n",
    "params.epoch = 44\n",
    "params.reference_mode = 1\n",
    "params.reference_lambda = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = create_data_loaders(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/taloved/env/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/taloved/env/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/taloved/tal-lxc/tal/docker/MoDLsinglechannel/demo_modl_BeaChloe/recon_net_wrap.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cp = torch.load('./lsdir-2x+hq50k_vit_epoch_60.pt', map_location=self.device)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "#model_ft = models.resnet18(weights='DEFAULT').to(device).requires_grad_(False)\n",
    "#model_ft.fc = nn.Identity()\n",
    "#model_ft = models.vgg16(weights='DEFAULT').to(device)#.requires_grad_(False)\n",
    "from FSloss_wrap import VGGLoss,ResNet18Backbone,FeatureEmbedding,contrastive_loss,VGGPerceptualLoss\n",
    "#VGGloss = VGGLoss().to(device)\n",
    "VGGloss = VGGPerceptualLoss().to(device)\n",
    "#UFLoss = ResNet18Backbone().to(device)\n",
    "#UFLoss = VGGLoss().to(device)\n",
    "#UFLoss = models.vgg16(pretrained=True).features[:8+1].to(device)\n",
    "#UFLoss.eval()\n",
    "\n",
    "def extract_patches(images, patch_size=(20, 20), stride=(20, 20)):\n",
    "    # images: Tensor of shape (batch_size, 1, 180, 110)\n",
    "    patches = images.unfold(2, patch_size[0], stride[0]).unfold(3, patch_size[1], stride[1])\n",
    "    patches = patches.permute(0, 2, 3, 1, 4, 5).contiguous()\n",
    "    patches = patches.view(images.size(0), -1, 1, patch_size[0], patch_size[1])\n",
    "    return patches  # Returns patches of shape (batch_size, num_patches, 1, patch_size[0], patch_size[1])\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "patch_size = (20, 20)\n",
    "stride = (20, 20)  # Non-overlapping patches\n",
    "def feature_space_loss(features1, features2):\n",
    "    return F.mse_loss(features1, features2)\n",
    "def pad_image(images):\n",
    "    # images: Tensor of shape (batch_size, 1, 172, 108)\n",
    "    padded_images = F.pad(images, (6, 6, 4, 4), mode='constant', value=0)\n",
    "    return padded_images  # Shape will be (batch_size, 1, 180, 120)\n",
    "#modelLoss = ResNet18Backbone().to(device)\n",
    "#embedding_model = FeatureEmbedding(modelLoss).to(device)\n",
    "#memory_bank = torch.randn(16, 128)  # Assuming num_patches is the number of different patches stored.\n",
    "#memory_bank = nn.functional.normalize(memory_bank, p=2, dim=1)  # Normalize the memory bank vectors\n",
    "\n",
    "\n",
    "from vision_transformer import VisionTransformer\n",
    "net = VisionTransformer(\n",
    "  avrg_img_size=320,\n",
    "  patch_size = (10,10),\n",
    "  in_chans=1,\n",
    "  embed_dim=64,\n",
    "  depth=10,\n",
    "  num_heads=16\n",
    "\n",
    ")\n",
    "\n",
    "from recon_net import ReconNet\n",
    "model = UnrolledViT(params).to(device)\n",
    "#model2 = ReconNet(net).to(device)#.requires_grad_(False)\n",
    "#cp = torch.load('./lsdir-2x+hq50k_vit_epoch_60.pt', map_location=device)\n",
    "#model2.load_state_dict(cp['model_state_dict'])\n",
    "\n",
    "\"\"\"\n",
    "model.requires_grad_(False)\n",
    "\n",
    "for net in model.similaritynets:\n",
    "    net.param1.requires_grad_(True)\n",
    "    net.param2.requires_grad_(True)\n",
    "    #net.recon_net.net.head.requires_grad_(True)\n",
    "\"\"\"\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer=optimizer, \n",
    "    max_lr=0.0001,\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    epochs=params.epoch,\n",
    "    pct_start=0.01,\n",
    "    anneal_strategy='linear',\n",
    "    cycle_momentum=False,\n",
    "    base_momentum=0., \n",
    "    max_momentum=0.,\n",
    "    div_factor = 25.,\n",
    "    final_div_factor=1.,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4109434/218409125.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_file,map_location=device)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'UnrolledViT' object has no attribute 'recon_net'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(checkpoint_file,map_location\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 8\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecon_net\u001b[49m\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastmri\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlosses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SSIMLoss\n\u001b[1;32m     11\u001b[0m criterion \u001b[38;5;241m=\u001b[39m SSIMLoss()\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1933\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'UnrolledViT' object has no attribute 'recon_net'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "### Load for fine-tunning\n",
    "\n",
    "checkpoint_file = \"../modl_singlechannel_reference/checkpoints_ViTFuser_FT_new_sairam/model_31.pt\" \n",
    "checkpoint = torch.load(checkpoint_file,map_location=device)\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "from fastmri.losses import SSIMLoss\n",
    "criterion = SSIMLoss().to(device)\n",
    "criterionMSE = nn.MSELoss()\n",
    "#criterion = nn.L1Loss()\n",
    "\n",
    "epochs_plot = []\n",
    "losses_plot = []\n",
    "\n",
    "for epoch in range(params.epoch):\n",
    "    model.train()\n",
    "    avg_loss = 0.\n",
    "    running_loss = 0.0\n",
    "    for iter, data in enumerate(train_loader):\n",
    "        input,target,mask,reference = data\n",
    "        input = input.to(device).float()\n",
    "        target = target.to(device).float()\n",
    "        mask = mask.to(device)\n",
    "        reference = reference.to(device).float()\n",
    "        image = T.ifft2(input)\n",
    "        image = image.permute(0,3,1,2)\n",
    "\n",
    "        #print(f'image shape: {image.shape}')\n",
    "        #print(f'reference shape: {reference.shape}')\n",
    "        ## Target\n",
    "        target_image = target.permute(0,3,1,2) \n",
    "        #print(f'ref size: {reference_image.shape}')\n",
    "        real_part_tar = target_image[:,0,:,:].unsqueeze(1)\n",
    "        imag_part_tar = target_image[:,1,:,:].unsqueeze(1)\n",
    "        mag_tar = torch.sqrt(real_part_tar**2 + imag_part_tar**2).to(device)\n",
    "        ## Reference\n",
    "        ref_image = reference.permute(0,3,1,2) \n",
    "        real_part_ref = ref_image[:,0,:,:].unsqueeze(1)\n",
    "        imag_part_ref = ref_image[:,1,:,:].unsqueeze(1)\n",
    "        mag_ref = torch.sqrt(real_part_ref**2 + imag_part_ref**2).to(device)\n",
    "\n",
    "        \n",
    "        #print(f'Features target: {features_target.shape}')\n",
    "        im_out = model(input,reference)#.squeeze(3)\n",
    "        \"\"\"\n",
    "        real_part_input = image[:,0,:,:].unsqueeze(1)\n",
    "        imag_part_input = image[:,1,:,:].unsqueeze(1)\n",
    "        mag_input = torch.sqrt(real_part_input**2 + imag_part_input**2).to(device)\n",
    "        im_out = im_out.cpu().detach().permute(0,3,1,2).numpy()\n",
    "        print(f'Input shape: {mag_input.shape}')\n",
    "        print(f'out shape: {(np.abs(im_out)).shape}')\n",
    "        print(f'ref shape: {mag_ref.shape}')\n",
    "        print(f'tar shape: {(mag_tar.squeeze(0)).shape}')\n",
    "\n",
    "\n",
    "        concat = np.concatenate((mag_ref.cpu().detach().numpy()[0,0,:,:],mag_input.cpu().detach().numpy()[0,0,:,:],np.abs(im_out[0,0,:,:]),mag_tar.squeeze(0).cpu().detach().numpy()[0,0,:,:]),axis=1)\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.imshow(concat, cmap='gray')\n",
    "        plt.title('reference                         in                           out                       target   ')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \"\"\"\n",
    "        #loss = criterion(im_out,features_target)\n",
    "        # SSIM\n",
    "\n",
    "        maxval = torch.max(torch.cat((im_out,mag_tar.permute(0,2,3,1)),dim=1))\n",
    "        im_out = im_out.permute(0,3,1,2)\n",
    "\n",
    "        #features_out = vgg16_model(torch.cat((im_out,im_out,im_out),dim =1))\n",
    "        \n",
    "        #print(features_out.shape)\n",
    "        data_range = torch.tensor([maxval], device=device).view(1, 1, 1, 1).expand(im_out.size(0), im_out.size(1), im_out.size(2)-6, im_out.size(3)-6)\n",
    "        #print(mag_tar.shape)\n",
    "        #print(im_out.shape)\n",
    "        #print(data_range.shape)\n",
    "        # SSIM\n",
    "        #loss = criterion(im_out, mag_tar.to(device), data_range.to(device))\n",
    "        # pad:\n",
    "        im_out_pad = torch.cat((im_out,im_out,im_out),dim =1)/maxval\n",
    "        mag_tar_pad = torch.cat((mag_tar,mag_tar,mag_tar),dim =1)/maxval\n",
    "        #loss = nn.MSELoss()(model_ft.features(im_out_pad), model_ft.features(mag_tar_pad))\n",
    "        \n",
    "        # SSIM + style\n",
    "        #print(f'ssim is : {+ criterion(im_out, mag_tar.to(device), data_range.to(device))}')\n",
    "        #loss =  0.4*criterion(im_out, mag_tar.to(device), data_range.to(device)) + 3*VGGloss(im_out,mag_ref.to(device))/2000 + 0.5*criterion(im_out, mag_ref.to(device), data_range.to(device)) # For tests2\n",
    "        ## Try MSE instead of SSIM\n",
    "        loss =  1*criterionMSE(im_out, mag_tar.to(device)) + 3*VGGloss(im_out,mag_ref.to(device))/2000 + 0.06*criterion(im_out, mag_ref.to(device), data_range.to(device)) # For tests2\n",
    "        \n",
    "        # No Fuser\n",
    "        #loss =  1*criterion(im_out, mag_tar.to(device), data_range.to(device)) \n",
    "\n",
    "        # SSIM loss for grant\n",
    "        #loss = criterion(im_out, mag_tar.to(device), data_range.to(device))\n",
    "        #loss = criterionMSE(im_out,mag_tar.to(device))\n",
    "       \n",
    "        # L1\n",
    "        #loss = criterion(features_out, features_out)\n",
    "        # MSE\n",
    "        #loss = criterion(im_out,mag_tar.permute(0,2,3,1))\n",
    "        \n",
    "        running_loss = running_loss + loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        avg_loss = 0.99 * avg_loss + 0.01 * loss.item() if iter > 0 else loss.item()\n",
    "        if iter % 400 == 0:\n",
    "            logging.info(\n",
    "                f'Epoch = [{epoch:3d}/{params.epoch:3d}] '\n",
    "                f'Iter = [{iter:4d}/{len(train_loader):4d}] '\n",
    "                f'Loss = {loss.item():.4g} Avg Loss = {avg_loss:.4g}'\n",
    "            )\n",
    "    #Saving the model\n",
    "    exp_dir = \"checkpoints_Chloe_fuser/\"\n",
    "    if epoch % 43 == 0:\n",
    "        torch.save(\n",
    "            {\n",
    "                'epoch': epoch,\n",
    "                'params': params,\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'exp_dir': exp_dir\n",
    "            },\n",
    "            f=os.path.join(exp_dir, 'model_%d.pt'%(epoch))\n",
    "    )\n",
    "    running_loss = running_loss / len(train_loader)\n",
    "    #scheduler.step(running_loss)\n",
    "    scheduler.step()\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    print(f'Epoch {epoch+1}, Learning rate: {current_lr}')\n",
    "\n",
    "    #print(f'Epoch {epoch+1}, Learning rate: {scheduler.get_last_lr()[0]}')\n",
    "    # Append epoch and average loss to plot lists\n",
    "    epochs_plot.append(epoch)\n",
    "    losses_plot.append(running_loss)\n",
    "\n",
    "# Plotting the loss curve\n",
    "plt.figure()\n",
    "plt.plot(epochs_plot, losses_plot, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('SA unrolled with Reference L2 train Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(exp_dir, 'loss_plot_plato_down.png'))  # Save plot as an image\n",
    "\n",
    "# Save all_losses to a file for later comparison\n",
    "losses_file = os.path.join(exp_dir, 'all_losses.txt')\n",
    "with open(losses_file, 'w') as f:\n",
    "    for loss in losses_plot:\n",
    "        f.write(f'{loss}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
